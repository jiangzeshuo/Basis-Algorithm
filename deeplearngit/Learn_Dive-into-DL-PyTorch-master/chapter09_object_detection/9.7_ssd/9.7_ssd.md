# 9.7. 单发多框检测（SSD）

我们在前几节分别介绍了边界框、锚框、多尺度目标检测和数据集，下面我们基于这些背景知识来构造一个目标检测模型：单发多框检测（single shot multibox detection，SSD）[1]。它简单、快速，并得到了广泛应用。该模型的一些设计思想和实现细节常适用于其他目标检测模型。

## 9.7.1. 定义模型

图9.4描述了单发多框检测模型的设计。它主要由一个基础网络块和若干个多尺度特征块串联而成。其中基础网络块用来从原始图像中抽取特征，因此一般会选择常用的深度卷积神经网络。单发多框检测论文中选用了在分类层之前截断的VGG [1]，现在也常用ResNet替代。我们可以设计基础网络，使它输出的高和宽较大。这样一来，基于该特征图生成的锚框数量较多，可以用来检测尺寸较小的目标。接下来的每个多尺度特征块将上一层提供的特征图的高和宽缩小（如减半），并使特征图中每个单元在输入图像上的感受野变得更广阔。如此一来，图9.4中越靠近顶部的多尺度特征块输出的特征图越小，故而基于特征图生成的锚框也越少，加之特征图中每个单元感受野越大，因此更适合检测尺寸较大的目标。由于单发多框检测基于基础网络块和各个多尺度特征块生成不同数量和不同大小的锚框，并通过预测锚框的类别和偏移量（即预测边界框）检测不同大小的目标，因此单发多框检测是一个多尺度的目标检测模型。

![ssd示意图](https://s1.ax1x.com/2020/03/14/8Mjzcj.png)

接下来我们介绍如何实现图中的各个模块。我们先介绍如何实现类别预测和边界框预测。

### 9.7.1.1. 类别预测层

设目标的类别个数为 q 。每个锚框的类别个数将是 q+1 ，其中类别0表示锚框只包含背景。在某个尺度下，设特征图的高和宽分别为 h 和 w ，如果以其中每个单元为中心生成 a 个锚框，那么我们需要对 hwa 个锚框进行分类。如果使用全连接层作为输出，很容易导致模型参数过多。回忆“网络中的网络（NiN）”一节介绍的使用卷积层的通道来输出类别预测的方法。单发多框检测采用同样的方法来降低模型复杂度。

具体来说，类别预测层使用一个保持输入高和宽的卷积层。这样一来，输出和输入在特征图宽和高上的空间坐标一一对应。考虑输出和输入同一空间坐标 (x,y) ：输出特征图上 (x,y) 坐标的通道里包含了以输入特征图 (x,y) 坐标为中心生成的所有锚框的类别预测。因此输出通道数为 a(q+1) ，其中索引为 i(q+1)+j （ 0≤j≤q ）的通道代表了索引为 i 的锚框有关类别索引为 j 的预测。

首先引入必要的库

```python
import sys
sys.path.append("../../")
import d2lzh_pytorch as d2l

import os
import json
import time
import numpy as np
from tqdm import tqdm
from PIL import Image

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision.transforms as transforms
```

下面我们定义一个这样的类别预测层：指定参数 a 和 q 后，它使用一个填充为1的 3×3 卷积层。该卷积层的输入和输出的高和宽保持不变。

```python
# define classification prediction layer
def cls_predictor(input_channels, num_anchors, num_classes):
    return nn.Conv2d(in_channels=input_channels, out_channels=num_anchors * (num_classes + 1), kernel_size=3, padding=1)
```

### 9.7.1.2. 边界框预测层

边界框预测层的设计与类别预测层的设计类似。唯一不同的是，这里需要为每个锚框预测4个偏移量，而不是 q+1 个类别。

```python
# define bounding box prediction layer
def bbox_predictor(input_channels, num_anchors):
    return nn.Conv2d(in_channels=input_channels, out_channels=num_anchors * 4, kernel_size=3, padding=1)
```

### 9.7.1.3. 连结多尺度的预测

前面提到，单发多框检测根据多个尺度下的特征图生成锚框并预测类别和偏移量。由于每个尺度上特征图的形状或以同一单元为中心生成的锚框个数都可能不同，因此不同尺度的预测输出形状可能不同。

在下面的例子中，我们对同一批量数据构造两个不同尺度下的特征图Y1和Y2，其中Y2相对于Y1来说高和宽分别减半。以类别预测为例，假设以Y1和Y2特征图中每个单元生成的锚框个数分别是5和3，当目标类别个数为10时，类别预测输出的通道数分别为 5×(10+1)=55 和 3×(10+1)=33 。预测输出的格式为(批量大小, 通道数, 高, 宽)。可以看到，除了批量大小外，其他维度大小均不一样。我们需要将它们变形成统一的格式并将多尺度的预测连结，从而让后续计算更简单。

```python
def forward(x, block):
    return block(x)
Y1 = forward(torch.zeros((2, 8, 20, 20)), cls_predictor(8, 5, 10))
Y2 = forward(torch.zeros((2, 16, 10, 10)), cls_predictor(16, 3, 10))
print(Y1.shape, Y2.shape)
```

可以看到，我们模拟了不同尺度上特征图大小不同，且每个中心位置生成的锚框个数不同的情况。

```
torch.Size([2, 55, 20, 20]) torch.Size([2, 33, 10, 10])
```

通道维包含中心相同的锚框的预测结果。我们首先将通道维移到最后一维。因为不同尺度下批量大小仍保持不变，我们可以将预测结果转成二维的(批量大小, 高 × 宽 × 通道数)的格式，以方便之后在维度1上的连结。

```python
def flatten_pred(pred):
    # change (B,C,H,W) to (B,H,W,C) and flatten
    return pred.permute(0, 2, 3, 1).reshape(pred.size(0),-1)

def concat_preds(preds):
    return torch.cat(tuple([flatten_pred(p) for p in preds]), dim=1)

# concatenating predictions for Multiple Scales
concat_Y = concat_preds([Y1, Y2])
print(concat_Y.shape)
```

这样一来，尽管Y1和Y2形状不同，我们仍然可以将这两个同一批量不同尺度的预测结果连结在一起。

```
torch.Size([2, 25300])
```

### 9.7.1.4. 高和宽减半块

为了在多尺度检测目标，下面定义高和宽减半块down_sample_blk。它串联了两个填充为1的 3×3 卷积层和步幅为2的 2×2 最大池化层。我们知道，填充为1的 3×3 卷积层不改变特征图的形状，而后面的池化层则直接将特征图的高和宽减半。由于 1×2+(3−1)+(3−1)=6 ，输出特征图中每个单元在输入特征图上的感受野形状为 6×6 。可以看出，高和宽减半块使输出特征图中每个单元的感受野变得更广阔。

```python
# Height and Width Downsample Block
def down_sample_blk(input_channels, num_channels):
    blk = []
    for _ in range(2):
        blk.append(nn.Conv2d(in_channels=input_channels, out_channels=num_channels, kernel_size=3, padding=1))
        blk.append(nn.BatchNorm2d(num_features=num_channels))
        blk.append(nn.ReLU())
        input_channels=num_channels
    blk.append(nn.MaxPool2d(kernel_size=2, stride=2))
    blk = nn.Sequential(*blk)
    return blk

temp_Y = forward(torch.zeros((2, 3, 20, 20)), down_sample_blk(3, 10))
print(temp_Y.shape)
```

测试高和宽减半块的前向计算。可以看到，它改变了输入的通道数，并将高和宽减半。

```
torch.Size([2, 10, 10, 10])
```

### 9.7.1.5. 基础网络块

基础网络块用来从原始图像中抽取特征。为了计算简洁，我们在这里构造一个小的基础网络。该网络串联3个高和宽减半块，并逐步将通道数翻倍。当输入的原始图像的形状为 256×256 时，基础网络块输出的特征图的形状为 32×32 。

```python
# Base Network Block
def base_net():
    blk = []
    num_filters = [3, 16, 32, 64]
    for i in range(len(num_filters) - 1):
        blk.append(down_sample_blk(num_filters[i], num_filters[i+1]))
    blk = nn.Sequential(*blk)
    return blk

temp_Y = forward(torch.zeros((2, 3, 256, 256)), base_net())
print(temp_Y.shape)
```

可以看到，256*256的输入图像经过基础网络后尺寸变为：

```
torch.Size([2, 64, 32, 32])
```

### 9.7.1.6. 完整的模型

单发多框检测模型一共包含5个模块，每个模块输出的特征图既用来生成锚框，又用来预测这些锚框的类别和偏移量。第一模块为基础网络块，第二模块至第四模块为高和宽减半块，第五模块使用全局最大池化层将高和宽降到1。因此第二模块至第五模块均为图9.4中的多尺度特征块。

```python
# define feature blocks
def get_blk(i):
    if i == 0:
        blk = base_net()
    elif i == 1:
        blk = down_sample_blk(64, 128)
    elif i == 4:
        blk = nn.AdaptiveMaxPool2d((1,1))
    else:
        blk = down_sample_blk(128, 128)
    return blk
```

接下来，我们定义每个模块如何进行前向计算。与之前介绍的卷积神经网络不同，这里不仅返回卷积计算输出的特征图Y，还返回根据Y生成的当前尺度的锚框，以及基于Y预测的锚框类别和偏移量。

```python
# define create anchors function
import itertools
import math
def create_anchors(feature_map_size, step, sizes, ratios):
    """Compute default box sizes with scale and aspect transform.
    feature_map_size 特征图大小(这里约定h=w)
    steps 可以理解为将原图划分为feature_map_size * feature_map_size时每一个小格所占的像素大小
    sizes 可选择的anchor大小,以像素为单位
    ratios 可选择的宽高比，(w/h)
    """
    scale = 256.
    step = step / scale
    sizes = [s / scale for s in sizes]

    boxes = []
    for h, w in itertools.product(range(feature_map_size), repeat=2):  # itertools.product的作用相当于笛卡儿积
        # 计算特征图当前位置的anchor的中心坐标，不同size和ratio是共享的
        cx = (w + 0.5)*step
        cy = (h + 0.5)*step

        # 先计算宽高比1*1，不同size的anchor
        ar = 1
        for s in sizes:
            boxes.append((cx, cy, s, s))
            #boxes.append((cx, cy, (s * math.sqrt(ar)), (s / math.sqrt(ar))))

        # 再计算基础尺寸下，不同宽高比的anchor
        s = sizes[0]
        for ar in ratios:
            if ar != 1:  # 剔除ratio=1的anchor以避免重复
                boxes.append((cx, cy, (s * math.sqrt(ar)), (s / math.sqrt(ar))))

    return torch.Tensor(boxes) # [h*w*anchors, 4]


def blk_forward(X, blk, sizes, ratios, cls_predictor, bbox_predictor):
    Y = blk(X)
    feature_map_size = Y.size(2)  # 这里默认feature_map的h和w相等
    step = 256/feature_map_size
    anchors = create_anchors(feature_map_size, step, sizes, ratios)
    cls_preds = cls_predictor(Y)
    bbox_preds = bbox_predictor(Y)
    return (Y, anchors, cls_preds, bbox_preds)
```

我们提到，图9.4中较靠近顶部的多尺度特征块用来检测尺寸较大的目标，因此需要生成较大的锚框。我们在这里先将0.2到1.05之间均分5份，以确定不同尺度下锚框大小的较小值0.2、0.37、0.54等，再按$\\sqrt{0.2 \\times 0.37} = 0.272$、$\\sqrt{0.37 \\times 0.54} = 0.447$等来确定不同尺度下锚框大小的较大值。

```python
sizes = [[0.2*256, 0.272*256], [0.37*256, 0.447*256], [0.54*256, 0.619*256],
         [0.71*256, 0.79*256], [0.88*256, 0.961*256]] 
ratios = [[1, 2, 0.5]] * 5
num_anchors = len(sizes[0]) + len(ratios[0]) - 1
```

现在，我们就可以定义出完整的模型TinySSD了。

```python
# define the complete model: TinySSD
class TinySSD(nn.Module):
    def __init__(self, input_channels, num_classes):
        super(TinySSD, self).__init__()

        input_channels_cls = 128
        input_channels_bbox = 128
        self.num_classes = num_classes

        self.blk_0 = get_blk(0)    # backbone, output: (*,64,32,32)
        self.blk_1 = get_blk(1)    # downsample, output: (*,128,16,16)
        self.blk_2 = get_blk(2)    # downsample, output: (*,128,8,8)
        self.blk_3 = get_blk(3)    # downsample, output: (*,128,4,4)
        self.blk_4 = get_blk(4)    # global pooling, output: (*,128,1,1)

        self.cls_0 = cls_predictor(64, num_anchors, num_classes)
        self.cls_1 = cls_predictor(input_channels_cls, num_anchors, num_classes)
        self.cls_2 = cls_predictor(input_channels_cls, num_anchors, num_classes)
        self.cls_3 = cls_predictor(input_channels_cls, num_anchors, num_classes)
        self.cls_4 = cls_predictor(input_channels_cls, num_anchors, num_classes)

        self.bbox_0 = bbox_predictor(64, num_anchors)
        self.bbox_1 = bbox_predictor(input_channels_bbox, num_anchors)
        self.bbox_2 = bbox_predictor(input_channels_bbox, num_anchors)
        self.bbox_3 = bbox_predictor(input_channels_bbox, num_anchors)
        self.bbox_4 = bbox_predictor(input_channels_bbox, num_anchors)

    def forward(self, X):
        anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5

        X, anchors[0], cls_preds[0], bbox_preds[0] = blk_forward(X, self.blk_0, sizes[0], ratios[0],
                                                                    self.cls_0, self.bbox_0)

        X, anchors[1], cls_preds[1], bbox_preds[1] = blk_forward(X, self.blk_1, sizes[1], ratios[1],
                                                                    self.cls_1, self.bbox_1)

        X, anchors[2], cls_preds[2], bbox_preds[2] = blk_forward(X, self.blk_2, sizes[2], ratios[2],
                                                                    self.cls_2, self.bbox_2)

        X, anchors[3], cls_preds[3], bbox_preds[3] = blk_forward(X, self.blk_3, sizes[3], ratios[3],
                                                                    self.cls_3, self.bbox_3)

        X, anchors[4], cls_preds[4], bbox_preds[4] = blk_forward(X, self.blk_4, sizes[4], ratios[4],
                                                                    self.cls_4, self.bbox_4)
        total_anchors_num = 5444  # (32^2 + 16^2 + 8^2 + 4^2 + 1) * num_anchors 
        return (torch.cat(anchors, dim=0), concat_preds(cls_preds).reshape((-1, total_anchors_num, self.num_classes + 1)),
                concat_preds(bbox_preds))
```

我们创建单发多框检测模型实例并对一个高和宽均为256像素的小批量图像X做前向计算。我们在之前验证过，第一模块输出的特征图的形状为 32×32 。由于第二至第四模块为高和宽减半块、第五模块为全局池化层，并且以特征图每个单元为中心生成4个锚框，每个图像在5个尺度下生成的锚框总数为$(32^2 + 16^2 + 8^2 + 4^2 + 1)\\times 4 = 5444$。

```python
# now we can create a SSD model
def init_weights(m):
    if type(m) == nn.Linear or type(m) == nn.Conv2d:
        torch.nn.init.xavier_uniform_(m.weight)

net = TinySSD(3, num_classes=1)
net.apply(init_weights)

X = torch.zeros((32, 3, 256, 256))
anchors, cls_preds, bbox_preds = net(X)

print('output anchors:', anchors.shape)
print('output class preds:', cls_preds.shape)
print('output bbox preds:', bbox_preds.shape)
```

## 9.7.2. 训练模型

下面我们描述如何一步步训练单发多框检测模型来进行目标检测。

### 9.7.2.1. 读取数据集和初始化

如果你还没有准备好皮卡丘数据集，请阅读[目标检测数据集（皮卡丘）](https://github.com/monkeyDemon/Learn_Dive-into-DL-PyTorch/blob/master/chapter09_computer_vision/9.6_object_detection_dataset_Pikachu/9.6_object_detection_dataset_pikachu.md)一节的教程并运行如下代码一键完成数据集的创建

```python
d2l.download_and_preprocess_data()
```

首先我们读取目标检测数据集（皮卡丘）一节构造的皮卡丘数据集。

```python
batch_size = 32
data_dir = '../../dataset/pikachu/'
train_dataset = d2l.PIKACHU(data_dir, 'train')
val_dataset = d2l.PIKACHU(data_dir, 'val')

train_loader = torch.utils.data.DataLoader(train_dataset,
                                           batch_size=batch_size, shuffle=True,
                                           num_workers=4)

val_loader = torch.utils.data.DataLoader(val_dataset,
                                         batch_size=batch_size, shuffle=False,
                                         num_workers=4)
```

在皮卡丘数据集中，目标的类别数为1，每张图片中包含一只待检测的皮卡丘~。

定义好模型以后，我们需要初始化模型参数并定义优化算法。

```python
# TODO: choose gpu
os.environ["CUDA_VISIBLE_DEVICES"] = "7"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# init model 
net = TinySSD(3, num_classes=1)
net.apply(init_weights)
net = net.to(device)

learning_rate = 1e-3
weight_decay = 5e-4
optimizer = optim.SGD(net.parameters(), lr = learning_rate, weight_decay=weight_decay)
```

### 9.7.2.2. 定义损失函数和评价函数

目标检测有两个损失：一是有关锚框类别的损失，我们可以重用之前图像分类问题里一直使用的交叉熵损失函数；二是有关正类锚框偏移量的损失。预测偏移量是一个回归问题，但这里不使用前面介绍过的平方损失，而使用 L1 范数损失，即预测值与真实值之间差的绝对值。掩码变量bbox_masks令负类锚框和填充锚框不参与损失的计算。最后，我们将有关锚框类别和偏移量的损失相加得到模型的最终损失函数。


